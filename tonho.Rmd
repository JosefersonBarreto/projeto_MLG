
# Objetivo 

Objetivo é executar uma analise afim de buscar relações entre as variáveis e conseguir um melhor ajuste  para fazer determinadas predições , para esta análise vamos utilizar o pacote RStanArm  e o software Rstúdio 


# Metodologia

para está análise utilizaremos o pacote ** RStanArm**  e o banco de dados **forestfires**   contendo informações relevantes sobre íncendios ocorridos  na aréa do parque natural  Montesesinho Bragança(portugal) , este conjunto de dados contém 517 observações.

Variáveis:


7. Informações de atributo:

   Para mais informações, leia [Cortez e Morais, 2007].

   (1) **X** Coordenada espacial do eixo X no mapa do parque de Montesinho: 1 a 9
   (2) **Y** Coordenada espacial do eixo Y no mapa do parque de Montesinho: 2 a 9
   3. **month** - mês do ano: "jan" a "dec"
   4. **day** - dia da semana: "seg" a "sol"
   5. **FFMC** - Índice FFMC do sistema FWI: 18,7 a 96,20
   6. **DMC** - DMC do sistema FWI: 1,1 a 291,3
   7. **DC** - Índice DC do sistema FWI: 7,9 a 860,6
   8. **ISI** - Índice ISI do sistema FWI: 0,0 a 56,10
   9. **temp** - temperatura em graus Celsius: 2,2 a 33,30
   10. **RH** - umidade relativa em %: 15,0 a 100
   11. vento - velocidade do vento em km/h: 0,40 a 9,40
   12. **wind** - chuva externa em mm/m2: 0,0 a 6,4
   13. **área** - a área queimada da floresta (em ha): 0,00 a 1090,84



# introdução


Os modelos  lineares  Generalizados são utilizados quando utilizar os modelos clássicos  que  pressupoem que os dados seguem uma distribuição  normal  e principalmente onde temos dados que segue uma base de dados discretos . O princípio do GLM é pegar uma família de distribuições  ao ínves de uma única distribuição  que no nosso caso será a famíla exponencial  , ou sejá , ao ínves de trabalhar apenas com a normal , trabalhamos com várias distribuições : gamma,expenencial,poisson ,binomial,binomial negativa,weibell 

Um estudo de regressão busca,essencialmente,associar uma variável **Y** (denominada variável dependente)a uma outra variável **X** (denominada variável explanatória ou variável independente ).



## Partes de um MLG 


(1) $Y_{i} \sim p(y | \theta)  \rightarrow E(y_{i}) = \mu_{i}$   
  
  
(2) $n_{i} = g(\mu_{i})$

  
  
(3) $n_{i} =  \beta_{0} + \beta_{1} *x_{I}$


# Conhecendo o Nossos Dados 

## Análise Exploratória 

O ponta pé  inicial de qualquer análise é a análise exploratória, com ela começas a entender a grandeza dos nossos dados e os seus comportamentos, dessa forma,  vamos carregar os pacotes necessários e executar a análise exploratória: 

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(DT)
library(readxl)


dados<-read.csv("forestfires.csv",sep = ",")








```

 Vamos verificar se nosso conjunto de dados pussui valores ausentes, caso tenhamos, teremos  que remover essas observações
 
 
```{r}
sum(dados[!complete.cases(dados),])
```
 
 Como podemos ver o nosso conjunto de dados não apresentou nenhum valor faltante, logo, podemos prosseguir da forma que está.
 
 
###  Observando a Grandeza dos  Nossos Dados 

Para verificar a grandeza dos nossos dados vamos utilizar o seguinte comando:
```{r}
glimpse(dados)
```
 Como podemos ver  a maioria das variáveis já estão no formato adequado,mas ainda teremos que tratar duas variáveis:  **Month,Day** 
 essas variáveis estão no formato texto ,vamos converte-las para fator 
 
 
### Transformações de Variáveis 
 
```{r}
 dados$month<-factor(dados$month,levels = c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"),labels = c("1","2","3","4","5","6","7","8","9","10","11","12"))



dados$day<-factor(dados$day,levels=c("sun","mon","tue","wed",
            "thu","fri","sat"),labels = c("1","2","3","4","5","6","7"))

```
 
 Com nossas variáveis devidamente ordenadas  transformadas em fatores podemos converte-las para númericas,ou seja" por exemplo  , iremos cronologicamente trabalahar com a variável **day** ,considerando: dia 1 , dia 2 ,dia 3 e assim sucessivamente. O mesmo pensamento vale para a variável  **month** onde iremos considerar: mês 1 ,mês 2 , mês 3 .... 
 
 <font face = "arial black" color="rgb(0, 153, 0)" > Observação: </font>   
 esse tipo de conversão não muda a grandeza de nossos dados , tendo em vista que a escala de medida   é de uma únidade para  **day** e **month** logo, o delta será uma unidade  em cada mês  e para a variável **day**, será uma unidade para cada dia. Nesse sentido , a correlação irá pegar se na escala ta crescendo ou decrescendo a medida que a outra variável se altera , logo podemos fazer está conversão.
 
 
 
```{r}
dados$day<-as.numeric(dados$day)

dados$month<-as.numeric(dados$month)






corrplot::corrplot(cor(dados[-3:-4]),type = "upper")



#library(dataMaid)
#dataMaid::makeDataReport(dados, output = 'html')

#help(makeDataReport)
```
 
 
```{python,echo=FALSE}
# import warnings
# warnings.filterwarnings("ignore")
# 
# 
# from bs4 import BeautifulSoup
# 
# import requests
# 
# html = requests.get("https://www.climatempo.com.br/").content
# 
# soup = BeautifulSoup(html, 'html.parser')
# 
# print(soup.prettify())
# 
# 
# temperatura = soup.find("span", class_="_block_margin-b-5 -gray")
# 
# 
# soup.find("span")
# print(temperatura.string)
```




```{python,echo=FALSE}
# #importe a biblioteca usada para consultar uma URL
# import urllib.request
# 
# #importe as funções BeautifulSoup para analisar os dados retornados do site
# from bs4 import BeautifulSoup
# 
# #especifique o URL
# wiki = "https://pt.wikipedia.org/wiki/Lista_de_capitais_do_Brasil"
# 
# #Consulte o site e retorne o html para a variável 'page'
# page = urllib.request.urlopen(wiki)
# 
# #Parse o html na variável 'page' e armazene-o no formato BeautifulSoup
# soup = BeautifulSoup(page, "html5lib")
# 
# #!pip install html5lib
# !pip3 install lxml
# !pip3 install  html5lib
# 
# soup = BeautifulSoup(page, features="xml")
```
```{python,echo=FALSE}
# from requests_html import HTMLSession
# 
# 
# session = HTMLSession()
# 
# #!pip  install requests_html
# 
# 
# requisicao= session.get("https://globoesporte.globo.com/futebol/brasileirao-serie-a/")
# 
# nome_times = requisicao.html.find(".club-name--desktop")
# 
# pontos = requisicao.html.find(".points")
# 
# 
# #print(nome_times[0].text)
# #print(pontos[0].text)
# 
# for item in range(20):
#   print(nome_times[item].text  + " " + pontos[item].text)
```





```{r,out.width='100%',background=rgb(43/255, 228/255, 178/255),warning=FALSE,message=FALSE,eval=FALSE}
library(dlookr)

eda_report(dados, "area", output_format = "html",browse = FALSE,output_dir = "C:/Users/joseferson/Documents/joseferson barreto/projeto_MLG")
```


```{r,out.width=800,background=rgb(43/255, 228/255, 178/255),warning=FALSE,message=FALSE}

library(knitr)
# Primeiro pacote: dataMaid
#library(dataMaid)
#dados<-py$pdados
#dataMaid::makeDataReport(dados, output = "HTML")
knitr::include_url("EDA_Report.html",height = '500')


```



##   Seleção de Atributos

Por que selecionar atributos?

Dentre os atributos, existem aquelas onde as informações contidas vão importar mais para o problema que você deseja resolver.

Mas por que existe essa diferença? Existem vários fatores que podem influenciar nessa questão, sendo os mais comuns:

1- Muitos dados faltantes naquela coluna, impossibilitando qualquer conclusão. Exemplo: Mais da metade das células em branco/vazias em uma coluna determinada.

2- Colunas com alta correlação entre si. Exemplo: Se estou trabalhando com colunas de carros, tendo uma coluna a quantidade de rodas e outra correspondente a quantidade de eixos no carro, temos uma correspondente total, pois cada eixo, há exatamente duas rodas. Assim, ao fazer essas análises, existem duas variáveis redundantes, aumentando a quantidade de dados a serem processados.

3- Muitos atributos. É ainda que na sua base de dados a ser trabalhada, existe um grande número de colunas, passando das milhares, ou mesmo, como em mais colunas do que alinhadas. Assim, haverá um grande custo de processamento, selecionar quais atributos deveríamos nos atentar,ajuda a minimizar este custo.

Aplicando o método de seleção de atributos


```{r}
library(randomForest)



# dividindo os dados em treino e teste 
set.seed(1234)
amostra = sample(2,517,replace=T, prob=c(0.8,0.2))
dadostreino = dados[amostra==1,]
dadosteste = dados[amostra==2,]
importancia  = randomForest(areat ~ ., data = dados1)

col = importance(importancia)
options(scipen=999) 
varImpPlot(importancia)
```
 Como Dc apresentou correlaçao com a variável DCM vamos ter que remove-la do modelo, assim como a variável mês 
```{r}
model1=glm(area~temp+DMC+day+RH+FFMC+wind+X+Y,family = poisson(link = "log"), data = dados)

m1 <- glm(area ~ ., family = poisson('log'), data = dados)
summary(m1)

install.packages('countreg')
par(mfrow=c(1,3))
plot(m1, which=c(2,4))
countreg::rootogram(m1)
install.packages("countreg", repos="http://R-Forge.R-project.org")

# glm
# oi<-cor(dados[-3])
# 
# summary(model1)
# 
# corrplot::corrplot(cor(dados[-3]),type = "upper")
# 
# cor(dados)
require(MASS)
model.negative.binomial <- glm.nb(area~temp+DMC+day+RH+FFMC+DC+wind+X+Y, data = dados)
 summary(model.negative.binomial)
# 
# model2=glm(area~temp+DMC+day+RH+FFMC+DC+wind+X+Y,family = quasibinomial(link = "logit"), data = dados)
# summary(m2 <- glm(area~temp+DMC+day+RH+FFMC+wind+X+Y, family =gaussian(link = "identity") , data = dados))
# 
# glm
# 
# summary(m1 <- glm.nb(area~temp+DMC+day+RH+FFMC+wind+X+Y,  data = dados))
# summary(model2)
# 
# model1=glm(area~temp+DMC+day+RH+FFMC+wind+X+Y, family = exponential, data = dados)
# shapiro.test(dados$area)
summary(model1)

library(survival)

teste<-survreg(Surv(day,area)~temp+DMC+RH+FFMC+DC+wind+X+Y, data = dados,dist='')

dau<-unique(dados$area)
summary(teste
)
AIC(model1,model.negative.binomial)

hist(unique(dados$area))
#intalar pacote 
install.packages("gamlss")

library(gamlss)

library(AICcmodavg)

#define list of models
models <- list( model.negative.binomial)
models<- list(model1)

#specify model names
mod.names <- c( 'disp.qsec')

#calculate AIC of each model
install.packages("AICcmodavg")

library(AICcmodavg)
aictab(cand.set = models, modnames = mod.names)


aictab()

Model selection based on AICc:
# modelo_NB <- gamlss(
#   formula = casos ~ cs(prec1, df = 8)+cs(tmax1, df = 8)+cs(hum3, df = 8)+cs(ao, df = 8)+cs(mes, df = 8)+cs(lncasos1, df = 8),
#   sigma.formula = ~ cs(prec1, df = 8)+cs(tmax1, df = 8)+cs(hum3, df = 8)+cs(ao, df = 8)+cs(mes, df = 8)+cs(lncasos1, df = 8),
#   family  = NBII,
#   mu.fix = TRUE,
#   sigma.fix = TRUE,
#   method = mixed(1,2),
#   data    = data,
#   trace   = FALSE
# )

gam

gamlss(area~temp+DMC+day+RH+FFMC+wind+X+Y, family = NBII , data = dados)


mod<-gamlss(area~pb(DMC),sigma.fo=~pb(DMC),family=PO, data=dados, method=mixed(1,20))

plot(mod)



library(gamlss)
modelo_PO <- gamlss(
  formula = area ~ cs(temp, df = 8)+cs(DMC, df = 8)+cs(day, df = 8)+cs(RH, df = 8)+cs(FFMC, df = 8)+cs(wind, df = 8),
  family  = PO,
  data    = dados,
  trace   = FALSE
)




ftd1 <- gamlss(area ~cs(FFMC, df = 8), family = DPO,data = dados)



library(ggfortify)
autoplot(model1)


```

```{r}
dados<-dados |> arrange(area)

dados<-dados[1:300,]


model1=glm(area~temp+DMC+day+RH+FFMC+wind+X+Y,family = poisson(link = "log"), data = dados)

```



```{python}
import pandas as pd 
from sklearn.datasets import load_boston
boston = load_boston() 


dataset = pd.DataFrame(boston.data, columns = boston.feature_names)
dataset['target'] = boston.target
```




```{r}
library(reticulate)
dt<-py$dataset


write.csv(dt,"boston.csv",row.names = F)
hist(dt$LSTAT)

shapiro.test(dt$LSTAT)
```

