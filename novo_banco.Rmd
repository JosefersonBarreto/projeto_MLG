---
title: <p style="text-align:center"><img src="https://www.infoescola.com/wp-content/uploads/2018/05/UEPB.png" width="1200" height="360" /></p> Modelos Lineares Generalizados
author: "Joseferson da Silva barreto e Antonio Victor"
date: "2022-09-30"
output:
 html_document:
    toc: true
    toc_float: true
    css: www/meu_cs.css
---



# Objetivo 

Objetivo é executar uma analise afim de buscar relações entre as variáveis e conseguir um melhor ajuste  para fazer determinadas predições , para esta análise vamos utilizar o pacote RStanArm  e o software Rstúdio 


# Metodologia

para está análise utilizaremos o pacote ** RStanArm**  e o banco de dados **forestfires**   contendo informações relevantes sobre íncendios ocorridos  na aréa do parque natural  Montesesinho Bragança(portugal) , este conjunto de dados contém 517 observações.

Variáveis:


# Definindo o Problema de Negócio
Nosso objetivo é construir um modelo de Machine Learning que seja capaz de fazer previsões sobre a taxa média de ocupação de casas na região de Boston, EUA, por proprietários. A variável a ser prevista é um valor numérico que representa a mediana da taxa de ocupação das casas em Boston. Para cada casa temos diversas variáveis explanatórias. Sendo assim, podemos resolver este problema empregando Regressão Linear Simples ou Múltipla.

# Definindo o Dataset
Usaremos o Boston Housing Dataset, que é um conjunto de dados que tem a taxa média de ocupação das casas, juntamente com outras 13 variáveis que podem estar relacionadas aos preços das casas. Esses são os fatores como condições socioeconômicas, condições ambientais, instalações educacionais e alguns outros fatores semelhantes. Existem 506 observações nos dados para 14 variáveis. Existem 12 variáveis numéricas em nosso conjunto de dados e 1 variável categórica. O objetivo deste projeto é construir um modelo de regressão linear para estimar a taxa média de ocupação das casas pelos proprietários em Boston.



# introdução


Os modelos  lineares  Generalizados são utilizados quando utilizar os modelos clássicos  que  pressupoem que os dados seguem uma distribuição  normal  e principalmente onde temos dados que segue uma base de dados discretos . O princípio do GLM é pegar uma família de distribuições  ao ínves de uma única distribuição  que no nosso caso será a famíla exponencial  , ou sejá , ao ínves de trabalhar apenas com a normal , trabalhamos com várias distribuições : gamma,expenencial,poisson ,binomial,binomial negativa,weibell 

Um estudo de regressão busca,essencialmente,associar uma variável **Y** (denominada variável dependente)a uma outra variável **X** (denominada variável explanatória ou variável independente ).



## Partes de um MLG 


(1) $Y_{i} \sim p(y | \theta)  \rightarrow E(y_{i}) = \mu_{i}$   
  
  
(2) $n_{i} = g(\mu_{i})$

  
  
(3) $n_{i} =  \beta_{0} + \beta_{1} *x_{I}$


# Conhecendo o Nossos Dados 

## Análise Exploratória 

O ponta pé  inicial de qualquer análise é a análise exploratória, com ela começas a entender a grandeza dos nossos dados e os seus comportamentos, dessa forma,  vamos carregar os pacotes necessários e executar a análise exploratória: 

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(DT)
library(readxl)


dados<-read.csv("boston.csv")








```

 Vamos verificar se nosso conjunto de dados pussui valores ausentes, caso tenhamos, teremos  que remover essas observações
 
 
```{r}
sum(dados[!complete.cases(dados),])
```
 
 Como podemos ver o nosso conjunto de dados não apresentou nenhum valor faltante, logo, podemos prosseguir da forma que está.
 
 
###  Observando a Grandeza dos  Nossos Dados 

Para verificar a grandeza dos nossos dados vamos utilizar o seguinte comando:
```{r}
glimpse(dados)
```
 Como podemos ver  a maioria das variáveis já estão no formato adequado,mas ainda teremos que tratar duas variáveis:  **Month,Day** 
 essas variáveis estão no formato texto ,vamos converte-las para fator 
 
 
### Transformações de Variáveis 
 
```{r}
 dados$month<-factor(dados$month,levels = c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec"),labels = c("1","2","3","4","5","6","7","8","9","10","11","12"))
t<-glm(dados$target~.,family = Gamma(link = "inverse"),data = dados)

glm
autoplot(t)

t2<-glm(dados$target~.,family = gaussian(link = "identity"),data = dados)


summary(t2)




t3<-glm(dados$target~.,family =quasipoisson(link = "log"),data = dados)


summary(t3)

autoplot(t3)
dados$day<-factor(dados$day,levels=c("sun","mon","tue","wed",
            "thu","fri","sat"),labels = c("1","2","3","4","5","6","7"))

model.negative.binomial <- glm.nb(target~., data = dados)
 summary(model.negative.binomial)

 autoplot(model.negative.binomial)
```
 
 Com nossas variáveis devidamente ordenadas  transformadas em fatores podemos converte-las para númericas,ou seja" por exemplo  , iremos cronologicamente trabalahar com a variável **day** ,considerando: dia 1 , dia 2 ,dia 3 e assim sucessivamente. O mesmo pensamento vale para a variável  **month** onde iremos considerar: mês 1 ,mês 2 , mês 3 .... 
 
 <font face = "arial black" color="rgb(0, 153, 0)" > Observação: </font>   
 esse tipo de conversão não muda a grandeza de nossos dados , tendo em vista que a escala de medida   é de uma únidade para  **day** e **month** logo, o delta será uma unidade  em cada mês  e para a variável **day**, será uma unidade para cada dia. Nesse sentido , a correlação irá pegar se na escala ta crescendo ou decrescendo a medida que a outra variável se altera , logo podemos fazer está conversão.
 
 
 
```{r}
dados$day<-as.numeric(dados$day)

dados$month<-as.numeric(dados$month)






corrplot::corrplot(cor(dados[-3:-4]),type = "upper")



#library(dataMaid)
#dataMaid::makeDataReport(dados, output = 'html')

#help(makeDataReport)
```
 
 
```{python,echo=FALSE}
# import warnings
# warnings.filterwarnings("ignore")
# 
# 
# from bs4 import BeautifulSoup
# 
# import requests
# 
# html = requests.get("https://www.climatempo.com.br/").content
# 
# soup = BeautifulSoup(html, 'html.parser')
# 
# print(soup.prettify())
# 
# 
# temperatura = soup.find("span", class_="_block_margin-b-5 -gray")
# 
# 
# soup.find("span")
# print(temperatura.string)
```




```{python,echo=FALSE}
# #importe a biblioteca usada para consultar uma URL
# import urllib.request
# 
# #importe as funções BeautifulSoup para analisar os dados retornados do site
# from bs4 import BeautifulSoup
# 
# #especifique o URL
# wiki = "https://pt.wikipedia.org/wiki/Lista_de_capitais_do_Brasil"
# 
# #Consulte o site e retorne o html para a variável 'page'
# page = urllib.request.urlopen(wiki)
# 
# #Parse o html na variável 'page' e armazene-o no formato BeautifulSoup
# soup = BeautifulSoup(page, "html5lib")
# 
# #!pip install html5lib
# !pip3 install lxml
# !pip3 install  html5lib
# 
# soup = BeautifulSoup(page, features="xml")
```
```{python,echo=FALSE}
# from requests_html import HTMLSession
# 
# 
# session = HTMLSession()
# 
# #!pip  install requests_html
# 
# 
# requisicao= session.get("https://globoesporte.globo.com/futebol/brasileirao-serie-a/")
# 
# nome_times = requisicao.html.find(".club-name--desktop")
# 
# pontos = requisicao.html.find(".points")
# 
# 
# #print(nome_times[0].text)
# #print(pontos[0].text)
# 
# for item in range(20):
#   print(nome_times[item].text  + " " + pontos[item].text)
```





```{r,out.width='100%',background=rgb(43/255, 228/255, 178/255),warning=FALSE,message=FALSE,eval=FALSE}
library(dlookr)

eda_report(dados, "area", output_format = "html",browse = FALSE,output_dir = "C:/Users/joseferson/Documents/joseferson barreto/projeto_MLG")
```


```{r,out.width=800,background=rgb(43/255, 228/255, 178/255),warning=FALSE,message=FALSE}

library(knitr)
# Primeiro pacote: dataMaid
#library(dataMaid)
#dados<-py$pdados
#dataMaid::makeDataReport(dados, output = "HTML")
knitr::include_url("EDA_Report.html",height = '500')


```



##   Seleção de Atributos

Por que selecionar atributos?

Dentre os atributos, existem aquelas onde as informações contidas vão importar mais para o problema que você deseja resolver.

Mas por que existe essa diferença? Existem vários fatores que podem influenciar nessa questão, sendo os mais comuns:

1- Muitos dados faltantes naquela coluna, impossibilitando qualquer conclusão. Exemplo: Mais da metade das células em branco/vazias em uma coluna determinada.

2- Colunas com alta correlação entre si. Exemplo: Se estou trabalhando com colunas de carros, tendo uma coluna a quantidade de rodas e outra correspondente a quantidade de eixos no carro, temos uma correspondente total, pois cada eixo, há exatamente duas rodas. Assim, ao fazer essas análises, existem duas variáveis redundantes, aumentando a quantidade de dados a serem processados.

3- Muitos atributos. É ainda que na sua base de dados a ser trabalhada, existe um grande número de colunas, passando das milhares, ou mesmo, como em mais colunas do que alinhadas. Assim, haverá um grande custo de processamento, selecionar quais atributos deveríamos nos atentar,ajuda a minimizar este custo.

Aplicando o método de seleção de atributos


```{r}
library(randomForest)



# dividindo os dados em treino e teste 
set.seed(1234)
amostra = sample(2,517,replace=T, prob=c(0.8,0.2))
dadostreino = dados[amostra==1,]
dadosteste = dados[amostra==2,]
importancia  = randomForest(areat ~ ., data = dados1)

col = importance(importancia)
options(scipen=999) 
varImpPlot(importancia)
```
 Como Dc apresentou correlaçao com a variável DCM vamos ter que remove-la do modelo, assim como a variável mês 
```{r}
# model1=glm(area~temp+DMC+day+RH+FFMC+wind+X+Y,family = poisson(link = "log"), data = dados)
# 
# m1 <- glm(area ~ ., family = poisson('log'), data = dados)
# summary(m1)
# 
# install.packages('countreg')
# par(mfrow=c(1,3))
# plot(m1, which=c(2,4))
# countreg::rootogram(m1)
# install.packages("countreg", repos="http://R-Forge.R-project.org")
# 
# # glm
# # oi<-cor(dados[-3])
# # 
# summary(model1)
# 
# corrplot::corrplot(cor(dados[-3]),type = "upper")
# 
# cor(dados)



# 
# library(gamlss)
# modelo_PO <- gamlss(
#   formula = area ~ cs(temp, df = 8)+cs(DMC, df = 8)+cs(day, df = 8)+cs(RH, df = 8)+cs(FFMC, df = 8)+cs(wind, df = 8),
#   family  = PO,
#   data    = dados,
#   trace   = FALSE
# )
# 
# 
# 
# 
# ftd1 <- gamlss(area ~cs(FFMC, df = 8), family = DPO,data = dados)
# 
# 
# 
# library(ggfortify)
# autoplot(model1)


```

```{r}
# dados<-dados |> arrange(area)
# 
# dados<-dados[1:300,]
# 
# 
# model1=glm(area~temp+DMC+day+RH+FFMC+wind+X+Y,family = poisson(link = "log"), data = dados)

```



```{python}
# import pandas as pd 
# from sklearn.datasets import load_boston
# boston = load_boston() 
# 
# 
# dataset = pd.DataFrame(boston.data, columns = boston.feature_names)
# dataset['target'] = boston.target
```




```{r}
# library(reticulate)
# dt<-py$dataset
# 
# 
# write.csv(dt,"boston.csv",row.names = F)
# hist(dt$LSTAT)
# 
# shapiro.test(dt$LSTAT)
```

