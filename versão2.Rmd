---
title:  <a style="text-align:center"><img src="https://www.infoescola.com/wp-content/uploads/2018/05/UEPB.png" width="1200" height="360" /></a>Modelos Lineares Generalizados(MLG) <p />
author: "joseferson da silva barreto e Antônio Victor Alves Silva "
date: "2022-10-12"
output:
 html_document:
    toc: true
    toc_float: true
    css: www/meu_cs01.css
---


# Objetivo 

Objetivo é executar uma analise a fim de buscar relações entre as variáveis e conseguir um melhor ajuste  para fazer determinadas predições , para esta análise vamos utilizar o pacote RStanArm  e o software Rstúdio 


# Metodologia

para está análise utilizaremos  banco de dados **Automobile Data Set**   contendo informações relevantes sobre características dos carros 
Variáveis:


### Definindo o Problema de Negócio 


Nosso objetivo é construir um modelo de regressão generalizado  que seja capaz de explicar  quais características influenciam na precificação do automovel. A variável a ser prevista é um valor numérico que representa o preço de cada automovel observado. Para cada automóvel temos diversas variáveis explanatórias. Sendo assim, podemos buscar resolver este problema utilizando  MLG.

### Definindo o Dataset
Este conjunto de dados consiste em dados do Anuário Automotivo da Ward de 1985. Aqui estão as fontes

Fontes:

1) 1985 Modelo de Importação de Carro e Especificações de Caminhão, Anuário Automotivo de 1985 Ward.
2) Personal Auto Manuals, Insurance Services Office, 160 Water Street, Nova York, NY 10038
3) Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037


Esse conjunto de dados consiste em três tipos de entidades: (a) a especificação de um automóvel em termos de várias características, (b) sua classificação de risco de seguro atribuída, (c) suas perdas normalizadas em uso em comparação com outros carros. A segunda classificação corresponde ao grau em que o automóvel é mais arriscado do que seu preço indica. Os carros recebem inicialmente um símbolo de fator de risco associado ao seu preço. Então, se for mais arriscado (ou menos), esse símbolo é ajustado movendo-o para cima (ou para baixo) na escala. Os atuários chamam esse processo de "simbolização". Um valor de +3 indica que o automóvel é arriscado, -3 que provavelmente é bastante seguro.

O terceiro fator é o pagamento de perda média relativa por ano de veículo segurado. Este valor é normalizado para todos os automóveis dentro de uma determinada classificação de tamanho (pequeno de duas portas, carrinhas, desporto/especialidade, etc…), e representa a perda média por automóvel por ano.

Nota: Vários dos atributos no banco de dados podem ser usados como um atributo de "classe".


* total de variáveis: 26
* total de observações :205




# Introdução


```{r,echo=FALSE,out.height=400,out.width=600,fig.align = 'center'}
knitr::include_graphics("Lamborghini-Urus-Performante-1.png")


```

  
  O primeiro meio de transporte a fazer uso de um motor a gasolina para se locomover foi um automóvel que continha somente três rodas e foi criado no ano de 1885 por um alemão de nome Karl Benz.A partir de então teve início a corrida pela produção e venda de automóveis, iniciada por uma empresa francesa conhecida pelo nome de Panhard et Levassor. No ano de 1892, o conhecido Henry Ford fabricou seu primeiro carro, o Ford, na América do Norte.Estima-se que a quantidade  de autóveis no Brasil já ultrapassa a casa dos  11 milhões de veículos,  segundo o senado, Dados da Secretaria Nacional de Trânsito do Ministério da Infraestrutura indicam haver mais de 3,5 milhões de caminhões em circulação no Brasil e, desse total, cerca de 26% dos veículos possuem mais de 30 anos de fabricação, a expectativa é que esse número aumente ainda mais nós próximos anos. Neste material  buscaremos criar um modelo capaz de explicar quais fatores influenciar no preço do automóvel, para este fim sera utilizado os modelos lineares generalizados.  

   Os modelos  lineares  Generalizados são utilizados quando utilizar os modelos clássicos  que  pressupoem que os dados seguem uma distribuição  normal  e principalmente onde temos dados que segue uma base de dados discretos . O princípio do GLM é pegar uma família de distribuições  ao ínves de uma única distribuição  que no nosso caso será a famíla exponencial  , ou sejá , ao ínves de trabalhar apenas com a normal , trabalhamos com várias distribuições : gamma,expenencial,poisson ,binomial,binomial negativa,weibell.
Um estudo de regressão busca,essencialmente,associar uma variável **Y** (denominada variável dependente)a uma outra variável **X** (denominada variável explanatória ou variável independente ).



## Partes de um MLG 


(1) $Y_{i} \sim p(y | \theta)  \rightarrow E(y_{i}) = \mu_{i}$   
  
  
(2) $n_{i} = g(\mu_{i})$

  
  
(3) $n_{i} =  \beta_{0} + \beta_{1} *x_{I}$


# Conhecendo o Nossos Dados 


## Análise Exploratória 

O ponta pé  inicial de qualquer análise é a análise exploratória, com ela começas a entender a grandeza dos nossos dados e os seus comportamentos, dessa forma,  vamos carregar os pacotes necessários e executar a análise exploratória: 

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(DT)
library(readxl)

rm(list = ls())
dados<-read.table("dados.txt",sep = ",",header = TRUE)








```

 Vamos verificar se nosso conjunto de dados pussui valores ausentes, caso tenhamos, teremos  que remover essas observações
 
 
```{r}
sum(dados[!complete.cases(dados),])
```
 
 Como podemos ver o nosso conjunto de dados não apresentou nenhum valor faltante, logo, podemos prosseguir da forma que está.
 
 
###  Observando a Grandeza dos  Nossos Dados 

Para verificar a grandeza dos nossos dados vamos utilizar o seguinte comando:
```{r}
glimpse(dados)
```


 Como podemos ver  a maioria das variáveis não estão no formato adequado,vamos ter que
 converte-las  para fatores as que são referentes a fatores e numericos a referente a valores númericos
 
 
 
 
### Transformações de Variáveis 

Vamos utilizar a função **mutate_if** para colocar a condição de transformação das colunas, por exemplo , no primeiro código abaixo  as culanas de dados da coluna 3 a coluna 9  que são do tipo character serão convertidas para factor , os demais códigos abaixo seguem as mesmas linhas de raciocínio.

```{r,warning=FALSE,message=FALSE}


dados$stroke[dados$stroke == "?"] <- "NA"
dados$bore[dados$bore == "?"] <- "NA"
dados$normalized.losses[dados$normalized.losses == "?"] <- "NA"
dados$num.of.doors[dados$num.of.doors == "?"] <- "NA"
dados$engine.size[dados$engine.size == "l"] <- "NA"
dados$price[dados$price == "?"] <- "NA"

dados[3:9]<-dados[3:9] %>% mutate_if(is.character,as.factor)
  
dados[15:16]<-dados[15:16] %>% mutate_if(is.character,as.factor) 
  
 

dados[18]<-dados[18] %>% mutate_if(is.character,as.factor)  


dados[20]<-dados[20] %>% mutate_if(is.character,as.factor) 



dados<-dados %>% mutate_if(is.character,as.numeric)



```

Vamos observar os nossos dados 

```{r,warning=FALSE,message=FALSE,out.width=200,out.height=400}
library(gt)
# dados |> datatable(
#           rownames = FALSE,
#           options = list(
#             columnDefs = list(list(className = 'dt-center', targets = 0:4))
#             )
#           )






dados |> head(10) |> gt()
```


Podemos perceber que após as transformações temos a presença de valores ausentes,teremos que tratar esses valores ausentes 


### Trantando valores ausentes nas variáveis númericas 
```{r}


# symboling é fator ,mas vai permanecer como númerica   


# subiistituindo os valores ausentes pelo valor mediano 

dados[is.na(dados$normalized.losses),]$normalized.losses =median(dados$normalized.losses,na.rm = T)

dados[is.na(dados$price),]$price = median(dados$price,na.rm = T)


dados$stroke<-as.numeric(dados$stroke)

#dados[is.na(dados$stroke),]$stroke = median(dados$stroke,na.rm = T)


dados[is.na(dados$bore),]$bore = median(dados$bore,na.rm = T)
#dados[is.na(dados$compression.ratio),]$compression.ratio = median(dados$compression.ratio,na.rm = T)
dados[is.na(dados$horsepower),]$horsepower= median(dados$horsepower,na.rm = T)

dados[is.na(dados$peak.rpm),]$peak.rpm= median(dados$peak.rpm,na.rm = T)




```


### Verificando se Ainda temos Algum Valor Ausente 

```{r}

colnms <- colnames(dados)

# filter
teste<-dados %>%
  filter_at(vars(all_of(colnms)), any_vars(is.na(.)))

sum(teste)
```
 Como podemos ver não temos mais a presença de valores ausentes, podemos prosseguir. 

Vamos observar novamente nossos dados 

```{r,warning=FALSE,message=FALSE,out.width=200,out.height=400}
library(gt)
# dados |> datatable(
#           rownames = FALSE,
#           options = list(
#             columnDefs = list(list(className = 'dt-center', targets = 0:4))
#             )
#           )






dados |> head(10) |> gt()
```


### Histograma e Normalidade

```{r}
library(ggplot2)
dados %>% ggplot(aes(price))+
  geom_density(fill = "darkblue")+
  labs(x= "Preço", y="Densidade")
```

 Observe que os dados de preço são assimétricos a esquerda.

```{r}
shapiro.test(dados$price)
```

 Pelo teste de normalidade univariada de Shapiro-Wilk, o valor deu menor que 0.05, então os dados de preço não seguem a normalidade dos resíduos.

### Vendo as Correlações das Variáveis Númericas 

Problemas de multicolinearidade são muitos comuns em modelos de regressão,isso acaba prejudicando o modelo, devido a este fato  vamos verificar a correção entre as variáveis preditoras, e depois vamos verificar quais variaveis remover, para verificar a correlação vamos executar  o gráfico de correlação  de pearson  através do comando abaixo:

```{r}

library(corrplot)
par(bg = '#586573')
dados[-26] |>
  dplyr::select(where(is.numeric))  |>
  cor( ) |>
corrplot::corrplot( type = "upper")
```


Como podemos ver várias colunas apresentaram correlação,logo  teremos que remove-lás 



### Verificando Presença de Multicolinearidade das Variáveis Númericas 


```{r,warning=FALSE,message=FALSE}
library(caret)

t<-dados[-26] |>
  dplyr::select(where(is.numeric)) 

 t %>%
  cor() %>%
  findCorrelation( cutoff = .50, verbose = FALSE)
```

Logo, essas as variáveis cotadas para remoção ,mas antes disso vamos ver se temos colunas 
com repetidas


### Verificando se Temos Colunas Identicas 

```{r,warning=FALSE,message=FALSE}
t %>%
  cor() %>%  findLinearCombos ()

```
Como podemos ver não temos colunas repetidas , vamos remover as colunas sugeridas 


```{r,warning=FALSE,message=FALSE}
t<-dados |>
  dplyr::select(where(is.numeric))  
   
library(tidyverse)
novo<-dados %>% 
  dplyr::select(symboling,normalized.losses,compression.ratio,horsepower,peak.rpm,price )

nov1<-dados |>
  dplyr::select(where(is.factor))  


#unido os dados 

 novo<-cbind(nov1,novo)
# 
# glimpse(novo)
# novo<-novo |>
#   dplyr::filter(make !="?" , fuel.type !="?",aspiration  !="?" ,
#                 num.of.doors !="?", engine.type !="l")


write.csv(novo,"carros2Trat.csv",row.names = F)
```

Agora podemos proseguir para criação do nosso modelo 








# Criando o Modelo MLG
Após a remoção das variáveis com problemas de multicolinearidade vamos criar o primeiro modelo utilizando todas as variáveis, o segundo  modelo recebera as variaveis selecionadas ,
```{r,warning=FALSE,message=FALSE}
# nv<- novo[11:16]
# 
mod<-glm(price~.,family = Gamma(link=log),data = novo)



summary(mod)


library(hnp)

hnp(mod$residuals, sim = 99,resid.type ='deviance',how.many.out=T ,conf = 0.95,scale = T)
```
 Podemos ver que várias  covariáveis não apresentaram significância , logo não são importantes para o modelo.
 
 
Para determinar o melhor modelo vamos utilizar o critério informação  de Akaike  que é uma métrica que mensura a qualidade de um modelo estatístico visando também a sua simplicidade. Fornece, portanto, uma métrica para comparação e seleção de modelos, em que menores valores de AIC representam uma maior qualidade e simplicidade, segundo este critério.





## Verificando a Importancia das Variáveis para o Modelo

Vamos verificar quais variáveis são mais importantes para o modelo , para isso vamos utilizar o pacote e os comandos abaixo:

```{r,warning=FALSE,message=FALSE}

library(randomForest)
importancia  = randomForest(price ~ ., data = novo)

col = importance(importancia)
options(scipen=999) 
par(bg = '#586573')
varImpPlot(importancia)
```

 Podemos perceber  que **make** apresenta maior importncia , seguida por **horsepower**   entre outras , as cováriaveis  com importância próximo a 0 serão removidas: (fuel.type,num.of.doors,aspiration).
 
 
 
```{r}
mod2<-glm(price~make+horsepower+num.of.cylinders+drive.wheels+fuel.system+
      compression.ratio+engine.type+body.style+peak.rpm,family = poisson(link=log), data = novo)

mod3<-glm(price~make+horsepower+num.of.cylinders+drive.wheels+fuel.system+
      compression.ratio+engine.type+body.style+peak.rpm,family = poisson(link = "identity"), data = novo)
summary(mod2)


require(MASS)
mod.nb <- glm.nb(price~make+horsepower+num.of.cylinders+drive.wheels+fuel.system+
      compression.ratio+engine.type+body.style+peak.rpm, data = novo)

summary(mod.nb)

library(DescTools)
PseudoR2(mod2)
PseudoR2(mod3)

hnp(mod2, sim = 99,resid.type ='deviance',how.many.out=T ,conf = 0.95,scale = T)


mod2<-glm(price~make+horsepower+num.of.cylinders+fuel.system+
      compression.ratio+body.style+engine.type,poisson(link = "log"), data = novo)

require(hnp)
hnp(mod2, sim = 99,resid.type ='deviance',how.many.out=T ,conf = 0.95,scale = T)

# trabalhando com o preço transformado
novo2<-novo


novo2$price<-log(novo$price)


mod2<-glm(price~make+horsepower+num.of.cylinders+drive.wheels+fuel.system+
      compression.ratio+engine.type+body.style+peak.rpm,family = poisson(link = "log"), data = novo2)



boxplot(scale(novo$price))
```
 
 






##  Tabela  de modelos
```{r,warning=FALSE,message=FALSE}
# library(hnp)
# 
# ajuste = c('mod', 'mod2','mod3','yu','yu2','mod.nb')
# aic    = c(AIC(mod),AIC(mod2),AIC(mod3),AIC(yu),AIC(yu2),AIC(mod.nb))
# deviance    = c(deviance(mod),deviance(mod2),deviance(mod3),deviance(yu),deviance(yu2),deviance(mod.nb))
# #verossimilhanca = c(logLik(glmod1),logLik(glmod2),logLik(glmod3))
# df<-data.frame(ajuste, round(aic,2),round(deviance,2)) 
# colnames(df)=c("ajuste","AIC","Deveiance")
# 
# df%>% datatable()


```


Como podemos ver  pelo O critério de informação de Akaike (AIC) e o deviance, o modelo que apresentou os melhores resultados segundo os critérios foi o modelo **mod**, que o modelo que contem variáveis removidas anteriormente,entretanto ao verificar o gráfico de envelope simulado normalmente distribuído para os resíduos, notamos que temos vários pontos fora do envelope, logo mesmo o  critério de Akaike (AIC) e o deviance sendo o mais baixo, o modelo aparenta não ser o mais indicado 












# Conclusão 

Como podemos ver mesmo o modelo **mod** sendo o melhor qualificado nas metricas do critério de informação de Akaique e na Deviance o grafico de envelope dos resíduos apresentou varios pontos  fora do envelope, logo, não é o modelo mais indicado para explicar nossa variável de interesse,sendo assim escolhemos o modelo **binomial negativo** embora não tenha apresentado o melhor AIC nem a melhor deviance  pussui o melhor R2, sendo assim o melhor modelo para explicar o preço dos carros. 

## Exibindo os Coeficientes do Nosso Modelo 

```{r}
mod.nb$coefficients
```



# Referências 

 O primeiro automóvel-INFO.Escola , acessado em 10/10/2022
 <https://www.infoescola.com/curiosidades/historia-do-automovel/>

1985 Modelo de Importação de Carro e Especificações de Caminhão, Anuário Automotivo de 1985 Ward.

Personal Auto Manuals, Insurance Services Office, 160 Water Street, Nova York, NY 10038

Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037


 dados veículos- IBGE ,acessado em  09/10/2022
 <https://cidades.ibge.gov.br/brasil/pesquisa/22/28120>


